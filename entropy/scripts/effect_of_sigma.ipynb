{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "sns.set(style=\"whitegrid\")\n",
    "import matplotlib\n",
    "label_size = 9\n",
    "matplotlib.rcParams['xtick.labelsize'] = label_size\n",
    "matplotlib.rcParams['ytick.labelsize'] = label_size\n",
    "y_formatter = matplotlib.ticker.ScalarFormatter(useOffset=False)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project root path\n",
    "PROJECT_ROOT = os.path.abspath(\n",
    "    os.path.join('..', '..'))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from entropy.modules.correlated_gaussians import CorrelatedGaussians\n",
    "from entropy.modules.matrix_estimator import MatrixEstimator\n",
    "from entropy.utils import constants\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2D data\n",
    "n_samples = 100\n",
    "\n",
    "data = CorrelatedGaussians()\n",
    "x_samples, y_samples = data.sample(dimension=1, corr_factor=0.0, n_samples=n_samples)\n",
    "z_samples = np.concatenate([x_samples, y_samples], axis=1).astype(np.float32)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=100)\n",
    "ax.scatter(z_samples[:, 0], z_samples[:, 1])\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-4, 4])\n",
    "ax.set_ylim([-4, 4])\n",
    "ax.set_title(\n",
    "    '2D Gaussian data\\n%d samples' \n",
    "    % n_samples, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of entropy of 2D variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show here the effect of the gaussian kernel width ($\\sigma$) on the estimation. Note that we have the following properties for the estimator regarding $\\sigma$ (proof is not shown here)\n",
    "\n",
    "$$\\lim_{\\sigma\\to 0} S(X) = \\lim_{\\sigma\\to 0} I(X;Y) = \\ln(N)$$\n",
    "$$\\lim_{\\sigma\\to \\infty} S(X) = \\lim_{\\sigma\\to \\infty} I(X;Y) = 0$$\n",
    "\n",
    "Where $N$ is the number of samples. We see that the estimator is bounded from below and from above. We say that the estimator is \"saturated\" if we reach the upper bound.\n",
    "\n",
    "Since the estimator relies on the entropy of the discrete probability distribution defined by the $N$ eigenvalues of the normalized Gram matrix of $N\\times N$ (obtained from the kernel evaluation on every pair of samples), we can interpret this result in the following way:\n",
    "\n",
    "- If $\\sigma$ is too small, the estimation converges to a maximum entropy case, which means that the values of the eigenvalues tend to be uniform. That is, each eigenvalue is equal to one another, and equal to a value of $1/N$.\n",
    "- If $\\sigma$ is too large, the estimation converges to a minimum entropy case, which means that the distribution tends to collapse to a \"single bin\". That is, a single eigenvalue has all the possible weight (equal to 1), while the rest has value of 0.\n",
    "\n",
    "Additionally, we can interpret $\\sigma$ as some sort of \"bin size\", if we were talking about histogram estimates.\n",
    "- The larger the value of $\\sigma$, the larger is the number of samples that end up in a bin, with the limiting case of all samples in one single bin (collapse). \n",
    "- The smaller the value of $\\sigma$, the smaller is the number of samples that end up in a bin, with the limiting case of only one sample per bin, leaving every bin with equal size (uniformity).\n",
    "\n",
    "We can expect that a proper tuning of $\\sigma$ is needed to avoid both extremes and have meaningful estimates and comparisons between different variables. It can be proved that, without proper care, changes in variance or changes in the number of dimensions can increase or decrease the \"effective\" sigma, introducing artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saturation value of estimator log(number of samples): %1.4f' % np.log(n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_zero_list = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    len(sigma_zero_list), 2, \n",
    "    figsize=(13, 2*len(sigma_zero_list)), \n",
    "    gridspec_kw = {'width_ratios':[1, 3]})\n",
    "\n",
    "for i, sigma_zero in enumerate(sigma_zero_list):\n",
    "    # Compute gram matrix\n",
    "    tf.reset_default_graph()\n",
    "    estimator = MatrixEstimator(sigma_zero, normalize_dimension=False, normalize_scale=False)\n",
    "    norm_gram = estimator.normalized_gram(z_samples)\n",
    "    with tf.Session() as sess:\n",
    "        norm_gram_np = sess.run(norm_gram)\n",
    "\n",
    "    # Compute eigenvalues\n",
    "    w, _ = np.linalg.eig(norm_gram_np)\n",
    "    w = np.real(w)\n",
    "    w = np.clip(w, 0, None)\n",
    "    w = w / w.sum()\n",
    "    w = -np.sort(-w)\n",
    "\n",
    "    alpha = 1.01  # Close to one to approximate Shannon\n",
    "    entropy = np.log(np.sum(w ** alpha)) / (1.0 - alpha)\n",
    "\n",
    "    ax[i, 0].set_title('Normalized Gram')\n",
    "    ax[i, 0].imshow(norm_gram_np, vmin=0, vmax=1/n_samples, cmap='gray')\n",
    "    ax[i, 0].grid(False)\n",
    "    ax[i, 0].set_xticks([])\n",
    "    ax[i, 0].set_yticks([])\n",
    "    ax[i, 0].set_ylabel('$\\sigma=%s$' % sigma_zero, fontsize=16)\n",
    "\n",
    "    ax[i, 1].bar(np.arange(w.size), w, 1, label='Estimated Entropy %1.4f' % (entropy))\n",
    "    ax[i, 1].set_ylabel('Value of Eigenvalue')\n",
    "    ax[i, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
